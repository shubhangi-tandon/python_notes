{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- Documented\n",
    "    - Datasets\n",
    "    - DataLoader and Multi-Processing in Pytorch\n",
    "    - DataLoader and Memory-Pining in PyTorch\n",
    "\n",
    "- Todo\n",
    "    - Torchscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "(Documentation from pytorch)\n",
    "\n",
    " torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
    " \n",
    "____len____ so that len(dataset) returns the size of the dataset.\n",
    "\n",
    "**__getitem__** to support the indexing such that dataset[i] can be used to get iith sample\n",
    "also implement how to read data from ____init____\n",
    "\n",
    "This is a __map__ style dataset. We cam also have an iterable sytle dataset where we implemrt the ____iter____ protocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text,label\n",
      "aaaa,0\n",
      "bbbb,1\n",
      "cccc,1\n",
      "dddd,0\n",
      "eeee,0\n",
      "table,1"
     ]
    }
   ],
   "source": [
    "# we will be creating a dataset for the following sample file. \n",
    "! cat ../data/sample_text_label.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample dataset to read a file which has two cols : (text, label)\n",
    "# we have to define two methods : __len__ and __get_item__ \n",
    "\"\"\"\n",
    " torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
    "__len__ so that len(dataset) returns the size of the dataset.\n",
    "__getitem__ to support the indexing such that dataset[i] can be used to get ith sample\n",
    "\"\"\"\n",
    "# we also define init to read the input file and/or apply some text(or image) tranformations \n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset??\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer=None, max_length=128):\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            csv_file (string) : Path to the csv file with data\n",
    "            transform(callable, optional) : Optional tranform to be applied on a sample\n",
    "        \"\"\"\n",
    "        df= pd.read_csv(csv_file)\n",
    "        #drop cols which are n/a (we are not adding the names of cols but if they are not added as headers , we can add it)\n",
    "        \n",
    "        df.dropna(subset =[\"text\", \"label\"], inplace = True)\n",
    "        \n",
    "        if tokenizer :\n",
    "            df[\"text\"] =df[\"text\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length))\n",
    "#         self.text = df[\"text\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length)).to_numpy()\n",
    "        self.text = df[\"text\"].to_numpy()\n",
    "        self.labels = df[\"label\"].to_numpy()\n",
    "       \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.labels[index]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('table', 1)\n",
      "CLS, embedding for table, SEP\n",
      "([0, 466, 1], 1)\n"
     ]
    }
   ],
   "source": [
    "# implementation -> \n",
    "# Huggingface : read a dataset using the test file we have . Initialize a tokenizer from BERT (with vocab file)\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "\n",
    "\n",
    "#1 -> simple dataset with text and label (no tokenizing)\n",
    "dataset = MyDataset(\"../data/sample_text_label.csv\")\n",
    "print(dataset[-1])\n",
    "\n",
    "#2 > huggingface -> read and apply tokenizer for model\n",
    "pretrained_model_path =(\"eBERT-base-titles-uncased-v2/\")\n",
    "tok = BertTokenizer(pretrained_model_path + \"vocab.txt\")\n",
    "dataset = MyDataset(\"sample_text_label.csv\", tok)\n",
    "print (\"CLS, embedding for table, SEP\")\n",
    "print(dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader\n",
    "# - inputs are -> Dataset (from above) \n",
    "from torch.utils.data import DataLoader\n",
    "# to see the signature : uncomment this line \n",
    "# DataLoader?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader and Multi-Processing in Pytorch\n",
    "- meaning of collate : collect and combine\n",
    "- Collate function -> to collate samples into batches. Torch has customized collation possible (and is often used)\n",
    "    - When automatic batching is disabled : fn called with individual data point, converts numpy -> pytorch\n",
    "    - When automatic batching is enabled : list of data samples at a time, expected to combine input samples into a batch for yielding from data loader generator \n",
    "    - Always preprend a new dimension as batch dimension\n",
    "    - numpy arrays -> Pytorch tensors\n",
    "    - Preserves the data structure ,but batched (if enabled)\n",
    "- Multi-processing\n",
    "    - Python Global Interpeter Lock prevents fully parallelizinf code across threads, pytorch provides an easy switch to perform multi-process data loading by setting num_workers to a positive int\n",
    "    - Use single process data loading by default -> data fetching done in the same process that DataLoader is initialized. Preferred when resource used for sharing data is limited or entired dataset can be loaded in memory\n",
    "        - more readability, good for debugging  \n",
    "    - Pytorch dataloaders give much faster data acess than the regular I/O performed on disk , because of multi-processing: num_workers is customizable\n",
    "        - once num_workers > 0, dataset, collate, worker_init_fn are passed to each worker\n",
    "        - torch.utils.data.get_worker_info returns useful info in a worker process\n",
    "        - map style -> main process generates indices using sampler and send to workers. Shuffle/randomization done in the main process\n",
    "        - recommended not to return CUDA tensors in multi-processing\n",
    "    - \n",
    "- Should be used to handle large datasets\n",
    "- default batch size =1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting pre-trained TF Bert model to Pytorch Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.json', 'pytorch_model.bin']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),]\n",
    "# for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
    "    # Load pretrained model/tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(pretrained_weights)\n",
    "model.save_pretrained(\"../models/pytorch_bert_uncased/\")\n",
    "\n",
    "os.listdir(\"../models/pytorch_bert_uncased/\")\n",
    "#  save_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_pytorch",
   "language": "python",
   "name": "tf_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
