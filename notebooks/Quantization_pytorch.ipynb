{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## What is optimization needed for ?\n",
    "Optimization is measured in terms of two components :\n",
    "1. Latency - How much time will it take for you to cross a bridge (or for a computation to complete)\n",
    "2. Throughput - How many such queries can we execute at the same time (or how many people can cross a bridge at the same time)\n",
    "\n",
    "## [PyTorch Optimization tricks](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)\n",
    "Quantization leverages 8bit integer instrictions to reduce model size and run inference faster \n",
    "\n",
    "1. Pytorch has data types correspondng to quantized tensots\n",
    "2. We can write kernels with quantinsed tensors,  torch.nn.quantized and torch.nn.quantized.dynamic name-space\n",
    "3. Quantized models are traceable and scriptable\n",
    "4. Mapping floating point tensor to quantize dtensors is customizable with user defined/fake quantization blocks (so that model trains with floats but knows to ultimately convert the model to int8)\n",
    "\n",
    "\n",
    "### Dynamic quantization\n",
    "1. Convert weights to int8 (Weights Only ). Good for LSTMs/Transformers and MLPs wirh small batch size . \n",
    "    1. 2X faster computer, 4x less memory\n",
    "2. convert activations to int8 on the fly (just before doing computation) [ like tanh, reulu etc] \n",
    "    a. Computations will be performed using int8 mult, faster compute (so only while doing the computation , we convert actications to int8 )\n",
    "    b. activations are read and written to memory in floating point\n",
    "3. use torch.quantization to convert modle to a dynamic quantized model \n",
    "\n",
    "\n",
    "### Post-training static quantization\n",
    "1. Improve performance = latency : Weights and Activations (8 bit) .\n",
    "    1. Works best for CNNS , accuracy is good. Requires Dataset calibration\n",
    "    2. Tweak Model, Calibrate on data, convert \n",
    "    3. Quantize wright and activations for entire model or submodules \n",
    "    4. 1.5 - 2x faster compute, 4x less memory\n",
    "2. First feeding batches of data through network and compute resulting distributions of different activations (by inserting observer modules at different points to record there distributions )\n",
    "3. This info is used to determine how different activsations should be quantized at inference time \n",
    "    1. A Simple technice is to divide the entire range of activations into 256 levels , but more sophisticated methods are also supported\n",
    "    2. Allows us to pass quantized vals between operations instead of converting vals to floats and back to ints \n",
    "4. Different options to optimize static quantization\n",
    "    1. Observers : customize observer to specify how stats are collected prior to quantization to try out more advanced methods to quantize\n",
    "        1. Use Torch.quantixation.prepare\n",
    "    2. Operator fusion : fuse multiple ops into a single op (saving on memory access and also improve acc) \n",
    "        1. Use torch.quantization.fuse_modules\n",
    "    3. Per channel quantization : independently quantize weights for each output channel in a convolution/linear layer = higher accuracy with same speed \n",
    "    4. Quantization itself done using torch.quantization.convert\n",
    "\n",
    "### Quantization Aware Training \n",
    "1. Works on weights and activation, to be used during fine tuning  \n",
    "    1. Works for all, best accuracy v/s performance tradeoff  \n",
    "    2. Steps almost identical to post training\n",
    "    3. Specify a different qconfig and use prepare_qat\n",
    "    4. Train instead of calibrate (so while training itself we are quantizing the weights)\n",
    "2. Under the hood :\n",
    "    1. Fake quantization to mimic quantization in forward pass (can be customized)\n",
    "    2. Straight through estimator in backword pass\n",
    "    3. Batch Norm special handling :\n",
    "        1. Fold batch normalization to mimix inference during training (?)\n",
    "        2. Freeze batch norm stats update for improved accuracy during quantization aware training (like FastAI?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning a module \n",
    "import torch\n",
    "from torch.nn.utils.prune import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_pytorch",
   "language": "python",
   "name": "tf_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
