{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to add frequently used Python concepts -> \n",
    "Have connected this from docs, tutorials and mistakes made during coding "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1 -> changing python kernel to a virtual environment \n",
    "1. activate your virtual env \n",
    "source  venv/bin/activate\n",
    "2. install ipykernel\n",
    "pip install ipykernel\n",
    "3. install a new kernel \n",
    "ipython kernel install --user --name=tf_pytorch\n",
    "4. relaunch jupyter\n",
    "jupyter notebook\n",
    "5. Switch to a new kernel : tf_pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Log loss, aka logistic loss or cross-entropy loss.\n",
       "\n",
       "This is the loss function used in (multinomial) logistic regression\n",
       "and extensions of it such as neural networks, defined as the negative\n",
       "log-likelihood of a logistic model that returns ``y_pred`` probabilities\n",
       "for its training data ``y_true``.\n",
       "The log loss is only defined for two or more labels.\n",
       "For a single sample with true label yt in {0,1} and\n",
       "estimated probability yp that yt = 1, the log loss is\n",
       "\n",
       "    -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
       "\n",
       "Read more in the :ref:`User Guide <log_loss>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : array-like or label indicator matrix\n",
       "    Ground truth (correct) labels for n_samples samples.\n",
       "\n",
       "y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
       "    Predicted probabilities, as returned by a classifier's\n",
       "    predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
       "    the probabilities provided are assumed to be that of the\n",
       "    positive class. The labels in ``y_pred`` are assumed to be\n",
       "    ordered alphabetically, as done by\n",
       "    :class:`preprocessing.LabelBinarizer`.\n",
       "\n",
       "eps : float\n",
       "    Log loss is undefined for p=0 or p=1, so probabilities are\n",
       "    clipped to max(eps, min(1 - eps, p)).\n",
       "\n",
       "normalize : bool, optional (default=True)\n",
       "    If true, return the mean loss per sample.\n",
       "    Otherwise, return the sum of the per-sample losses.\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "labels : array-like, optional (default=None)\n",
       "    If not provided, labels will be inferred from y_true. If ``labels``\n",
       "    is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
       "    assumed to be binary and are inferred from ``y_true``.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "\n",
       "Returns\n",
       "-------\n",
       "loss : float\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import log_loss\n",
       ">>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n",
       "...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
       "0.21616...\n",
       "\n",
       "References\n",
       "----------\n",
       "C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
       "p. 209.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The logarithm used is the natural logarithm (base-e).\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Public/tfr/pytorch_experiments/venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to load the definition of a class/function in jupyter -> add ?? (or ? for code and no docs) and execute\n",
    "# other jupyter trips and tricks : https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "# eg for documentation on log_loss method in sklearn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   text  label\n",
      "0  aaaa      0\n",
      "1  bbbb      1\n",
      "2  cccc      1\n",
      "3  dddd      0\n",
      "4  eeee      0\n",
      "[0 1 1 0 0]\n",
      "['aaaa' 'bbbb' 'cccc' 'dddd' 'eeee']\n"
     ]
    }
   ],
   "source": [
    "# Pandas Tips\n",
    "\n",
    "# to convert df to numpy\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"sample_text_label.csv\")\n",
    "print(df)\n",
    "\n",
    "#convert to numpy but exclude index\n",
    "\n",
    "labels = df[\"label\"].to_numpy()\n",
    "text = df[\"text\"].to_numpy()\n",
    "print(labels)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_pytorch",
   "language": "python",
   "name": "tf_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
